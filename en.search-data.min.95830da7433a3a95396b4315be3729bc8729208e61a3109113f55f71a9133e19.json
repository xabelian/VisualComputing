[{"id":0,"href":"/VisualComputing/docs/Coreccion-de-color-para-Daltonismo/Modificacion-de-imagenes-para-personas-con-dificultades-en-la-percepcion-de-Color/","title":"Modificacion De Imagenes Para Personas Con Dificultades en La Percepcion De Color","section":"Docs","content":" Modificación de imagenes para personas con dificultades en la percepcion de color # Introducción # El daltonismo es la incapacidad para ver algunos colores en la forma normal.\nEste ocurre cuando hay un problema con los pigmentos en ciertas células nerviosas del ojo que perciben el color. Estas células se llaman conos y se encuentran en la capa de tejido sensible a la luz que recubre la parte posterior del ojo, llamada la retina.\nSi sólo falta un pigmento, usted puede tener dificultad para diferenciar entre el rojo y el verde, que es el tipo más común de daltonismo. Si falta un pigmento diferente, usted puede tener dificultad para ver los colores azul y amarillo. Las personas con daltonismo para los colores azul y amarillo con frecuencia tienen problemas para identificar también los colores rojos y verdes.\nLa forma más grave de daltonismo es la acromatopsia. Se trata de una rara afección en la cual una persona no puede ver ningún color, solamente sombras de gris.\nLa mayoría de los casos de daltonismo se deben a un problema genético. Muy pocas mujeres son daltónicas y aproximadamente 1 de cada 10 hombres sufren alguna forma de daltonismo.\nTipos de Daltonismo # Acromático: no hay percepción de ningún color, la visión es en blanco y negro con matices de gris. Es poco frecuente (1/100.000).\nMonocromático: solo se ve un color, en distintas tonalidades.\nDicromático: existen tres tipos:\nProtanopía, ausencia de fotoreceptores al rojo. La parte del espectro de colores que normalmente se ve rojo-verde, se ve gris. Deuteranopía, ausencia de fotoreceptores al verde. La parte del espectro de colores que normalmente se ve verde, se ve gris. Tritanopía, ausencia de fotoreceptores al azul, esta es una condición muy rara. Tricromático anómalo: el portador confunde los colores, y es la alteración que se presenta con más frecuencia. En la macula existen los tres tipos de fotoreceptores (para rojo, verde y azul) pero la captación de colores es irregular. Estos pacientes tienen percepción de los colores anormal, semejante a los dicromáticos pero menos acentuadas, dentro de este grupo se incluyen:\nProtanomalía (1% en hombres- 0.01 % en mujeres). Deuteranomalía es la más frecuente, (6% en hombres- 0.4% en mujeres). Tritanomalía la menos frecuente (0.01 en hombres- 0.01 % en mujeres). Entendiendo el problema # Para las personas que no tenemos dificultades para el reconocimiento de colores, entender el daltonismo muchas veces es simplemente creer que se confunde un color por otro y ya, pero esta condición, como pudimos ver anteriormente, tiene distintas variantes y no solo eso, tambien niveles. Una persona con daltonismo tricromático con deutoranopia en grado leve moderado, podria percibir los colores rojos de una forma no tan vívida\nSi mediante al procesamiento de imagenes queremos simular este tipo de daltonismo, lo primero que debemos recordar es que en una imagen el color de cada pixel esta compuesto por niveles de cada uno de los colores RGB.\nEntonces si dismunuimos el nivel del rojo y aumentamos el nivel del verde podriamos acercarnos a una recreación de este tipo de daltonismo\nDeteccion de Daltonismo # Existen diferentes formas para la detección del Daltonismo.\nTests de Ishihara # Este es el tipo más común de prueba de daltonismo. Un oftalmólogo (usualmente) le pedirá que mire una imagen formada por puntos de colores con un número o forma de diferente color en el medio. Si la forma se mezcla con el fondo y no puedes verla, es posible que el paciente tenga un tipo de daltonismo.\nDiferentes placas de color pueden verificar diferentes tipos de daltonismo.\nTest con Anomaloscopio # Esta prueba verifica si el paciente puede igualar el brillo de dos luces. Se mira a través de un ocular a 2 luces que tienen diferentes niveles de brillo. Se usan perillas para ajustar las luces y se trata de hacer que coincidan. Si no puede igualar el brillo de las 2 luces, es posible que el paciente sufra de daltonismo.\nTest de tonos # En una prueba de tono, Se entregan bloques de diferentes colores. Su oftalmólogo pide al paciente que las coloque en el orden del arcoíris, como de rojo a púrpura. Si tiene problemas para ponerlos en el orden correcto, es posible que tenga un tipo de daltonismo. Los oftalmólogos a menudo usan esta prueba para las personas que necesitan tener una visión del color muy precisa para sus trabajos, como fotógrafos o diseñadores.\nResultados # Simulando la vision de un paciente # Original Image with red predominance # Load Image let img; function setup() { createCanvas(600, 505); img = loadImage(\u0026#39;/VisualComputing/sketches/red_crayons.png\u0026#39;); } function draw() { image(img, 0, 0); } Modify Red to Green let img; function preload() { img = loadImage(\u0026#39;/VisualComputing/sketches/red_crayons.png\u0026#39;); } function setup() { createCanvas(600, 505); img.loadPixels(); // Se recorre cada pixel de la imagen for (let y = 0; y \u0026lt; img.height; y++) { for (let x = 0; x \u0026lt; img.width; x++) { // Lee el color de cada pixel let originalColor = img.get(x, y); // Modifica los colores rojo y verde. El azul permanece intacto const r = red(originalColor)-50; const g = green(originalColor)+50; const b = blue(originalColor); let outputColor = color(r, g, b); // Coloca el nuevo color a cada pixel img.set(x, y, outputColor); } } img.updatePixels(); } function draw() { image(img, 0, 0); } Mejorando la visibilidad por medio de p5 # Se uso Coblis con la siguiente imagen para simular la vision de un paciente con Deuteranopia (ausencia de la percepción del espectro del color verde).\nA la izquierda la imagen original, a la derecha la imagen como la veria un paciente con Deuteranopia al ser simulado con Coblis.\nLa modificación de los valores de RGB permite aumentar el contraste de la imagen y diferencias las colores.\nModifying colors for visibility let img; function preload() { img = loadImage(\u0026#39;https://xabelian.github.io/VisualComputing/sketches/original.jpg\u0026#39;); } function setup() { createCanvas(404, 402); image(img, 0, 0, width, height); let d = pixelDensity(); loadPixels(); for (var y = 0; y \u0026lt; height*4; y++) { for (var x = 0; x \u0026lt; width; x++) { var index = (x + y * width)*4; var r = pixels[index+0]; var g = pixels[index+1]; var b = pixels[index+2]; var a = pixels[index+3]; if (g \u0026gt; 80){ pixels[index+2] = b+70 //pixels[index+1] = g-40 } } } updatePixels(); } Con este resultado, nuevamente simulamos usando Coblis y se obtiene la siguiente imagen.\nReferencias # How to make figures and presentations that are friendly to Colorblind people Understanding the concept of channels in an image\n"},{"id":1,"href":"/VisualComputing/docs/Procedural-Texturing/","title":"Procedural Texturing","section":"Docs","content":" Procedural Texturing # "},{"id":2,"href":"/VisualComputing/docs/Rendering/Scene-Trees/","title":"Scene Trees","section":"Docs","content":" Scene Trees # 3D Brush # Una de las aplicaciones mas interesantes sobre los entornos de dibujo 3D o 3D Brushes, es acerca de los distintos dispositivos que se pueden utilizar para manipular estos espacios de trabajo. La idea de utilizar dispositivos diferentes es mejorar la experiencia del usuario y permitirle utilizar dispositivos diferentes al teclado y el mosue.\nEn nuestro caso, estudiamos el uso de controles de consolas en un 3D Brush, puntualmente el control de un PlayStation 4. Se estudio el uso de la libreria GamepadAPI y drivers como DS4Windows\nPara la creación del 3D Brush, se utilizo como base el espacio dado por el profesor, haciendo uso de las librerias Treegl y Opencam.\np5 iFrame ShortCode \u0026lt; p5-iframe sketch=\u0026#34;/sketches/trees/3dbrush.js\u0026#34; lib1=\u0026#34;https://cdn.jsdelivr.net/gh/VisualComputing/p5.treegl/p5.treegl.js\u0026#34; lib2=\u0026#34;https://cdn.jsdelivr.net/gh/freshfork/p5.EasyCam@1.2.1/p5.easycam.js\u0026#34; width=\u0026#34;625\u0026#34; height=\u0026#34;475\u0026#34; \u0026gt; 3d Brush js // Goal in the 3d Brush is double, to implement: // 1. a gesture parser to deal with depth, i.e., // replace the depth slider with something really // meaningful. You may use a 3d sensor hardware // such as: https://en.wikipedia.org/wiki/Leap_Motion // or machine learning software to parse hand (or // body) gestures from a (video) / image, such as: // https://ml5js.org/ // 2. other brushes to stylize the 3d brush, taking // into account its shape and alpha channel, gesture // speed, etc. // Brush controls let color; let depth; let brush; let easycam; let state; let escorzo; let points; let record; function setup() { createCanvas(600, 450, WEBGL); // easycam stuff let state = { distance: 250, // scalar center: [0, 0, 0], // vector rotation: [0, 0, 0, 1], // quaternion }; easycam = createEasyCam(); easycam.state_reset = state; // state to use on reset (double-click/tap) easycam.setState(state, 2000); // now animate to that state escorzo = true; perspective(); // brush stuff points = []; depth = createSlider(0, 1, 0.05, 0.05); depth.position(10, 10); depth.style(\u0026#39;width\u0026#39;, \u0026#39;580px\u0026#39;); color = createColorPicker(\u0026#39;#ed225d\u0026#39;); color.position(width - 70, 40); // select initial brush brush = sphereBrush; } function draw() { update(); background(120); push(); strokeWeight(0.8); stroke(\u0026#39;magenta\u0026#39;); grid({ dotted: false }); pop(); axes(); for (const point of points) { push(); translate(point.worldPosition); brush(point); pop(); } } function update() { let dx = abs(mouseX - pmouseX); let dy = abs(mouseY - pmouseY); speed = constrain((dx + dy) / (2 * (width - height)), 0, 1); if (record) { points.push({ worldPosition: treeLocation([mouseX, mouseY, depth.value()], { from: \u0026#39;SCREEN\u0026#39;, to: \u0026#39;WORLD\u0026#39; }), color: color.color(), speed: speed }); } } function sphereBrush(point) { push(); noStroke(); // TODO parameterize sphere radius and / or // alpha channel according to gesture speed fill(point.color); sphere(1); pop(); } function keyPressed() { if (key === \u0026#39;r\u0026#39;) { record = !record; } if (key === \u0026#39;p\u0026#39;) { escorzo = !escorzo; escorzo ? perspective() : ortho(); } if (key == \u0026#39;c\u0026#39;) { points = []; } } function mouseWheel(event) { //comment to enable page scrolling return false; } 3D GUI # GUI utiliza mallas para crear una interfaz de usuario interactiva, que está completamente integrada en su escena. Es un programa informático que actúa de interfaz de usuario, utilizando un conjunto de imágenes y objetos gráficos para representar la información y acciones disponibles en la interfaz. Su principal uso consiste en proporcionar un entorno visual sencillo para permitir la comunicación con el sistema operativo de una máquina o computador.\nHabitualmente las acciones se realizan mediante manipulación directa, para facilitar la interacción del usuario con la computadora. Surge como evolución de las interfaces de línea de comandos que se usaban para operar los primeros sistemas operativos y es pieza fundamental en un entorno gráfico.\nLas GUI 3D aparecieron en la literatura y las películas de ciencia ficción antes de que fueran técnicamente viables o de uso común. En la ficción en prosa, las GUI 3D se han retratado como entornos sumergibles, acuñados como el \u0026ldquo;ciberespacio\u0026rdquo; de William Gibson y el \u0026ldquo;metaverso\u0026rdquo; y los \u0026ldquo;avatares\u0026rdquo; de Neal Stephenson.\nLa película estadounidense de 1993 Jurassic Park presenta el administrador de archivos 3D File System Navigator de Silicon Graphics, un administrador de archivos de la vida real para los sistemas operativos Unix. La película Minority Report tiene escenas de agentes de policía que utilizan sistemas de datos 3D especializados.\np5 iFrame ShortCode \u0026lt; p5-iframe sketch=\u0026#34;/sketches/trees/3dbrush.js\u0026#34; lib1=\u0026#34;https://cdn.jsdelivr.net/gh/VisualComputing/p5.treegl/p5.treegl.js\u0026#34; lib2=\u0026#34;https://cdn.jsdelivr.net/gh/freshfork/p5.EasyCam@1.2.1/p5.easycam.js\u0026#34; width=\u0026#34;625\u0026#34; height=\u0026#34;475\u0026#34; \u0026gt; 3D GUI js // Brush controls // texture let img; // check boxes let toggle_3d_gui; let auto_rotate; // select let mode; let tipo; // select forma let forma; // 3d gui let color1; let color2; //Ejes x,y,z let x_e = 0; let y_e = 0; let z_e = 0; let easycam; let foreshortening = true; // bulls shape let circled = false; // resume animation let frames = 0; // spaces let sphere1; let sphere2; let depth; function preload() { img = loadImage(\u0026#39;https://upload.wikimedia.org/wikipedia/commons/b/b1/Mapa_mundi_blanco.PNG\u0026#39;); } function setup() { createCanvas(600, 480, WEBGL); textureMode(NORMAL); toggle_3d_gui = createCheckbox(\u0026#39;toggle 3d gui\u0026#39;, false); toggle_3d_gui.style(\u0026#39;color\u0026#39;, \u0026#39;white\u0026#39;); toggle_3d_gui.position(10, 30); toggle_3d_gui.changed(() =\u0026gt; { if (toggle_3d_gui.checked()) { color1.show(); color2.show(); } else { color1.hide(); color2.hide(); } }); // brush stuff depth = createSlider(10, 130, 50, 0); depth.position(10, 10); depth.style(\u0026#39;width\u0026#39;, \u0026#39;270px\u0026#39;); // select initial brush // brush stuff depth1 = createSlider(10, 130, 50, 0); depth1.position(310, 10); depth1.style(\u0026#39;width\u0026#39;, \u0026#39;270px\u0026#39;); // select initial brush //eje de rotacion x x_rotate = createCheckbox(\u0026#39;X\u0026#39;, false); x_rotate.style(\u0026#39;color\u0026#39;, \u0026#39;white\u0026#39;); x_rotate.position(10, 90); //eje de rotacion y y_rotate = createCheckbox(\u0026#39;Y\u0026#39;, false); y_rotate.style(\u0026#39;color\u0026#39;, \u0026#39;white\u0026#39;); y_rotate.position(45, 90); //eje de rotacion z z_rotate = createCheckbox(\u0026#39;Z\u0026#39;, false); z_rotate.style(\u0026#39;color\u0026#39;, \u0026#39;white\u0026#39;); z_rotate.position(80, 90); //rotacion auto_rotate = createCheckbox(\u0026#39;reinicio rotate\u0026#39;, false); auto_rotate.style(\u0026#39;color\u0026#39;, \u0026#39;orange\u0026#39;); auto_rotate.position(10, 60); mode = createSelect(); mode.position(15, 120); mode.option(\u0026#39;Fill\u0026#39;); mode.option(\u0026#39;Wiredframe\u0026#39;); mode.option(\u0026#39;Texture\u0026#39;); mode.value(\u0026#39;Texture\u0026#39;); tipo = createSelect(); tipo.position(15, 150); tipo.option(\u0026#39;Esfera\u0026#39;); tipo.option(\u0026#39;Cilindro\u0026#39;); tipo.option(\u0026#39;Cono\u0026#39;); tipo.option(\u0026#39;Cubo\u0026#39;); tipo.option(\u0026#39;Cuboide\u0026#39;); tipo.option(\u0026#39;Piramide\u0026#39;); tipo.option(\u0026#39;Texture\u0026#39;); tipo.value(\u0026#39;Esfera\u0026#39;); color1 = createColorPicker(\u0026#39;cyan\u0026#39;); color2 = createColorPicker(\u0026#39;magenta\u0026#39;); easycam = createEasyCam(); let state = { distance: 300, // scalar center: [1, 0, 0], // vector rotation: [0, 0, 1, 0], // quaternion }; easycam.setState(state, 0); // animate to state over the period of 1 second } function draw() { background(100); push(); strokeWeight(2); stroke(\u0026#39;orange\u0026#39;); grid({size: 200}); pop(); axes(); if (x_rotate.checked()) x_e ++; if (y_rotate.checked()) y_e ++; if (z_rotate.checked()) z_e ++; if (auto_rotate.checked()) { x_e=0; y_e=0; z_e=0; } rotateX(x_e * 0.01); rotateZ(z_e * 0.01); rotateY(y_e * 0.01); axes(30); push(); switch (mode.value()) { case \u0026#39;Fill\u0026#39;: fill(255, 0, 0); break; case \u0026#39;Wiredframe\u0026#39;: noFill(); stroke(0, 255, 255); break; default: noStroke(); texture(img); } switch (tipo.value()) { case \u0026#39;Esfera\u0026#39;: sphere(depth.value()); break; case \u0026#39;Cilindro\u0026#39;: cylinder(depth1.value(), depth.value()*2); break; case \u0026#39;Cono\u0026#39;: cone(depth1.value(), depth.value()*1.5); break; case \u0026#39;Cubo\u0026#39;: box(depth.value()); break; case \u0026#39;Cuboide\u0026#39;: box(depth1.value(), depth.value()); break; case \u0026#39;Piramide\u0026#39;: cone(depth1.value(), depth.value(), 5); break; default: //sphere(depth.value(), 4, 20); //sphere(depth.value()); break; } // pop(); push(); translate(0, 50+depth.value()); rotateY(frames * 0.01); sphere1 = mMatrix(); axes(30); noStroke(); fill(color1.color()); sphere(15); pop(); push(); translate(0, -50-depth.value()); rotateZ(frames * 0.01); sphere2 = mMatrix(); axes(30); noStroke(); fill(color2.color()); sphere(15); pop(); if (toggle_3d_gui.checked()) { let sphere1Projection = treeLocation([0, 0, 0], { from: sphere1, to: \u0026#39;SCREEN\u0026#39; }); beginHUD(); color1.position(sphere1Projection.x, sphere1Projection.y); endHUD(); let sphere2Projection = treeLocation([0, 0, 0], { from: sphere2, to: \u0026#39;SCREEN\u0026#39; }); beginHUD(); color2.position(sphere2Projection.x, sphere2Projection.y); endHUD(); } } function keyPressed() { if (key === \u0026#39;b\u0026#39;) { circled = !circled; } if (key === \u0026#39;p\u0026#39;) { foreshortening = !foreshortening; foreshortening ? perspective() : ortho(); } } function mouseWheel(event) { //comment to enable page scrolling return false; } "},{"id":3,"href":"/VisualComputing/docs/Rendering/Software-Rendering/","title":"Software Rendering","section":"Docs","content":" Renderización # Rendering es el proceso de generar una imagen fotorrealista o no fotorrealista a partir de un modelo 2D o 3D por medio de un programa. La imagen resultante se conoce como el renderizado.\nSe pueden definir varios modelos en un archivo de escena que contiene objetos en un lenguaje o estructura de datos estrictamente definidos.\nEl archivo de escena contiene información de geometría, punto de vista, textura, iluminación y sombreado que describe la escena virtual. Los datos contenidos en el archivo de escena luego se pasan a un programa de renderizado para ser procesados y enviados a una imagen digital o un archivo de imagen de gráficos rasterizados.\nAlgo de historia # Hasta la década de los 90 no existía en el mercado de computadoras personales algo tal como las tarjetas aceleradoras o tarjetas gráficas. En los computadores Pentium 383 o 486, los gráficos eran producidos por renderización de software. Esto era suficiente para los juegos del momento, pero para mantenerse jugables, no tenían ningún tipo de texture filtering, o efectos.\nLuego con el advenimiento de las tarjetas gráficas S3 Virge y el 3DFX Voodoo, que permiten un filtro de texturas y un rendimiento de los gráficos 3D más rápido que el realizado por la renderización de software. En un comienzo, fue necesario reescribir el código de los juegos o sus motores para soportar los chips, pero para 1997 ya se contó con varias interfaces que permitian acceder a sus capacidades como DirectX y OpenGL.\nComo desde la perspectiva de la industria, la velocidad era donde se debían enfocar la industria de los gráficos por computadora y si bien la programación gráfica con renderización por software permitia una mayor flexibilidad en su modelo, esta ultima fue abandonada.\nDiferencias entre la renderización por software y por hardware # Sería ideal que los algoritmos de renderización por software permitieran una traducción directa al hardware, pero esto no es posible debido a los dos diferentes enfoques con los que trabajan.\nLa renderización por software mantiene la escena 3D a ser renderizada en memoria, y la muestra pixel por pixel o subpixel por subpixel. La renderización por software opera de otra forma. Los píxeles están presentes en todo momento, pero la renderización consiste en considerar la escena con un triángulo al momento, pintando cada uno de estos en el frame buffer, que es un buffer de memoria que contienen los datos que representan todos los pixeles en un videoframe. Las tarjetas de video contienen framebuffers en sus núcleos.\nEl hardware no tiene noción de la escena, solamente un solo triángulo es conocido en cada momento. En otras palabras la renderización por hardware o GPU el computador envía los datos geométricos a la GPU por medio de OpenGL o DirectX, allí la GPU comienza el procesamiento de esta geometría en pixeles por medio de shaders.\nEn la renderización software como el cómputo ocurre en la CPU, a diferencia del procesamiento de hardware, que se basa en la tarjeta gráfica de la máquina no está restringido por la tarjeta gráfica de la computadora; la representación de software generalmente es más flexible. Sin embargo, la contrapartida es que la renderización del software suele consumir más tiempo.\nEl método de bucle de subpixel es escogido por que permite renderizar efectos no locales que requieren considerar diferentes porciones de una escena en orden de computar un pixel o subpixel.\nPor ejemplo, la refleccion requiere acceso a tanto lo que refleja cómo el objeto reflejado. Un ejemplo más complejo es la iluminación global, que considera la luz indirecta de todos los objetos que rodean un objeto para poder computar el brillo de un subpixel siendo renderizado. Para resumir, la renderización por software usa el CPU para renderizar gráficos 3D.\nLa renderización por hardware no puede realizar ninguno de estos procesos por que no tiene ninguna noción de los objetos, sólo conoce un triángulo una vez al tiempo. Es el API el que se encarga de traducir estas nociones a la tarjeta gráfica.\nDe esta forma la renderización de hardware tienen un conjunto de \u0026ldquo;workarounds\u0026rdquo; para trabajar alrededor de estas limitaciones. Estos \u0026ldquo;workarounds\u0026rdquo; usualmente implican el pre-renderizar objetos en “mapas”, que son rectángulos que codifican las propiedades de otros objetos y que son almacenados en el hardware gráfico en forma de imágenes de textura.\nUn ejemplo son los mapas de reflexión: Se renderiza una escena desde el punto de vista de un espejo. La imagen es luego pegada cual el render final necesita saber que se ve en el espejo. Este método solo funciona en espejos planos o casi planos.\nLas soluciones como esta tienen una dificultad: requieren mucha preparación manual y ajustes para que funcionen; por ejemplo, decidir sobre un buen mapa ambiental. La computadora no puede hacer esto porque, si bien es posible calcular un mapa correcto, sería muy costoso y anularía fácilmente el ahorro de tiempo esperado.\nTipos de renderización. # La renderización en tiempo real tambien conocida como renderización online, es usada para renderizar una escena, como en los videojuegos 3D, donde generalmente cada frame debe ser renderizado en unos pocos milisegundos.\nEl enfoque de la renderización en tiempo real es entonces el performance. Uno de los primeros juegos similares a los juegos modernos 3D fue Descent de Parallax Software. Se distingue por permitir el 6DoF (Six degrees of freedom). El juego contiene modelos 3D hechos enteramente de polígonos triangulares con texturas de bitmap.\nPor otro lado tenemos la renderización offline, con la que nos referimos a cualquier cosa en la que los fotogramas se rendericen en un formato de imagen y las imágenes se muestren más tarde como una imagen fija o una secuencia de imágenes . Buenos ejemplos de renderizadores offline son el Hyperion de Disney y RenderMan de Pixar. Muchos de estos renderizadores de software hacen uso de lo que se conoce como un algoritmo de Ray-Tracing.\nRay-tracing # El trazado de rayos consiste en simular las interacciones entre la luz y la materia. Es hecho al enviar rayos de luz desde una fuente de luz hacia el mundo, y calculando la trayectoria basada en la naturaleza y la orientación de las superficies que son cruzadas por los rayos. Luego para cada uno de los rayos, el color es determinado en la localización, basado en la historia del rayo.\nEl RayTracing, hasta finales de la última década era una característica de los renderizadores offline, y no encontrábamos nada similar en un renderizador real-time.\nVentajas y desventajas de la renderización por Software # La síntesis por imágenes tiene muchas ventajas sobre la tecnología basada en GPU. Como la CPU realiza el proceso hay menos necesidad de preocuparnos sobre problemas de compatibilidad porque no es necesario adaptarnos a un hardware especial. La renderización es programada uniformemente usando el mismo lenguaje que la aplicación y no hay restricciones sobre la data (como el tamaño máximo de textura).\nLa desventaja principal es que toda la data es almacenada en la memoria principal. Para cualquier cambio en la data, el CPU necesita contactar la memoria y estas solicitudes están limitadas por el tiempo de acceso de cada tipo de memoria, cambios frecuentes en los datos de la memoria causan una pérdida de velocidad.\nEl segundo problema se origina del ancho de banda del bus (PCIe), es el movimiento de largas cantidades de datasets entre la memoria y la memoria de video. Durante un segundo la pantalla puede redibujar alrededor de 50 y 60 veces, lo que resulta en un significativo flujo de datos entre las dos memorias. En el caso de una resolucion baja, de 1024x768, con una profundidad de color de 32 bits, el buffer de la pantana mantiene alrededor de 3mb de datos.\nAplicaciones: Software rendering para peliculas de animacion 3D # La flexibilidad de la renderización por software ha sido fundamental en la producción de películas de animación 3D. Si bien en programas como los videojuegos, donde una generación instantánea de gráficos es vital, en la producción de fotogramas para películas, es más importante la calidad y posibilidad de modelar diferentes efectos. Por ejemplo, de acuerdo con Pixar, en 2020, declaró que les tomaba alrededor de 50 horas/CPU para renderizar un frame en una resolución de 2K.\nMesa Software Renderer, LLVMpipe y OpenSWR # El Mesa o Mesa 3D Graphics Library es una implementación de software libre de OpenGL, Vulkan y otras APIs gráficas. El propósito de Mesa\nEs portable lo cual le permite usar OpenGL en sistemas que no cuentan con una implementación de OpenGL La renderización por software de Mesa sirve como referencia para validar los drivers de hardware\nLa implementación de OpenGL es útil para experimentar, o para probar nuevas técnicas de renderización. Mesa permite renerizar imagenes con un canales digitales con una mayor profundidad de color (bits por pixel) como canales de 16 bits enteros o de 32 bits de punto flotante. Algo que solo hasta ahora aparece en hardware. Los límites internos de Mesa (máximas luces, tamaño de la textura, planos de recorte) pueden ser cambiados para necesidades especiales.\nMesa cuenta con varios renderizadores por software entre los que el más popular es el LLVMpipe. El Gallium LLVMpipe es un rasterizador por software que usa LLVM (Low Level Virtual Machine) para la generación de código en runtime.\nMesa tambien puede ser usado junto al rasterizador OpenSWR, que provee un rasterizador compatible con OpenGL para alto rendimiento y escalabilidad. OpenSWR está completamente basado en la CPU, corre en cualquier plataforma desde laptops hasta ambientes de alto rendimiento. OpenSWR se construye sobre LLVM y utiliza modernos sets de instrucciones como el Intel Advanced Vector Extensions para alcanzar los altos niveles de rendimiento que promete.\nBibliografía # [1] \u0026ldquo;Hardware vs. Software Rendering\u0026rdquo;, Autodesk, 2022. Recuperado de: https://download.autodesk.com/us/maya/2008help/refguide/node57.html.\n[2] \u0026ldquo;Software Rendering from Scratch\u0026rdquo;, Medium, 2022. Recuperado de: https://medium.com/@aminere/software-rendering-from-scratch-f60127a7cd58.\n[3] \u0026ldquo;Modern software rendering\u0026rdquo;, Production Systems and Information Engineering. 6. 55-66. Mileff, Peter \u0026amp; Dudra, Judit. (2012).\n"},{"id":4,"href":"/VisualComputing/docs/Texture-Sampling/","title":"Texture Sampling","section":"Docs","content":" Texture Sampling # "},{"id":5,"href":"/VisualComputing/docs/Uso-de-shaders-como-ayuda-para-personas-con-daltonismo/","title":"Uso De Shaders Como Ayuda Para Personas Con Daltonismo","section":"Docs","content":" Uso de Shaders para el ajuste de colores como ayuda para personas con daltonismo # Como se trabajó en la primera entrega de este curso, pudimos identificar distintos tipos de incapacidades en las personas para percibir distintos tipos de colores en las imágenes. Desde la Acromatopsia (Incapacidad para percibir cualquier color) hasta la Tritanopía (Dificualtades para percibir el azul), por medio de modificaciones y ajustes en las imágenes mediante sofware, primero es posible simular la discapacidad para que una persona sana entienda como ve el mundo una persona con este padecimiento y segundo, es posible realizar ajustes sobre la gama de colores pasra que la persona daltonica pueda percibir un poco mejor las imágenes.\nEn esa primera entrega se realizó la modificación de los valores RGB de las imágenes por medio de P5. Ahi se identificó que para cada uno de los tipos de daltonismo, era posibel modificar la escada de cada uno de los valores de RGB para el ajuste de las imágenes. Ahora, el objetivo es hacer uso de shaders para evaluar si con este método se pueden obtener mejores resultados.\n¿Y, que son los Shaders? # En pocas palabras, los shaders son programas que se ejecutan directamente en la placa de video de un computador, esto debido a que las GPU estan enfocadas a la matemática de vectores y matrcies, mientras que las CPU convencionales no lo están. En este curso se han trabajado 2 tipo principales de Shaders, los Vertex Shaders y los Fragment Shaders (también conocidos como los Píxel Shaders)\nVertex Shaders # Los Vertex Shaders se ejecutan una vez por cada vértice que forma parte del elemento que se quiere renderizar. Permiten hacer efectos sobre los propios vértices, es decir, moverlos para hacer algún efecto de distorsión. Su valor de retorno es la posición del vértice procesada\nFragment Shaders # Estos se ejecutan una vez por cada fragmento visible de la imagen (es decir, la cantidad de veces que se ejecuten depende de la vista de cámara, tamaño del objeto y otros factores más). Su valor de retorno es el color del pixel resultante. Con la ayuda de los vertex shaders, permiten hacer efectos vistosos como iluminación, cel shading, bump mapping, y una gran cantidad de filtros de post-procesamiento como desenfoque, profundidad de campo, desenfoque de movimiento, bloom, hdr, entre otros.\nEjemplo de uso de shaders # En este primer ejemplo tenemos como baes una geometria triangular, al desplazar el mouse sobre el canvas de izquiera a derecha se aumentan los numeros de vertices de la figura (vertez shader), a su vez que cambia el color de los pixeles al interior de la misma (fragment shader)\nEste segundo ejemplo es el mismo que se encuentra en la página del curso, en donde por medio del uso de coordenadas baricéntricas se obtienen los colores por interpolación dentro de un triangulo generado aleatoriamente. Al pasar el mouse sobre la imagen, cambian los colores de esta\nCómo implementar los shaders para mejora de reconocimiento de imagenes en el daltonimso? - Mejora de imagen desde una Web Cam # Como vimos anteriormente, los shaders nos permiten manipular los vertices y los pixeles de una imagen, esto es de gran utilidad ya que se pueden mejorar los contornos y realzar ciertos colores para mejorar su visualización. Esto atacaría directamente el problema de las personas con daltonismo, que es no poder identificar de manera adecuada una imagen que tenga determinados tipos de colores.\nSi se realiza un cambio en los colores de una imagen y se implementa una mejora en los contornos de la misma, las personas con discapacidades visuales podráin identificar las imágenes con mayor facilidad.\nEsta imagen muestra un ejemplo del resultado que se quiere obtener realizando el realce de colores y contornos, ahora, el objetivo es la implementación en un video en tiempo real con una web cam.\nImagen desde la cámara web # Conclusiones # La implementación de shaders para manipulación de iamgenes en tiempo real es posible gracias a todo el trabajo existente en OpenGL, ya que esto permite que con recursos de hardware, que incluso se encuentran en la nube de GitHub, se pueda hacer uso de los shaders. Sin embargo, los algoritmos requeridos para obtener el resultado final (mejoras de colores y sombreados) son mas complejos de lo esperado para la implementación en video, razón por la cual solo se logró la implementación de una máscara sobre la imagen obtenida de la cámara web.\nTrabajo futuro # El objetivo, y se continua desarrollando conocimiento e investigación sobre esta área, es seguir implementado shaders y máscaras para mejoramiento de video en tiempo real par ayudar a las personas con discapacidades visuales.\nReferencias # How to make figures and presentations that are friendly to Colorblind people How to make figures and presentations that are friendly to Colorblind people Introducción a Shaders GLSL Introducción a Shaders GLSL Applying Shaders to vertex Applying Shaders to vertex Using fragment shaders to manipulate images - Oregon State University Using fragment shaders to manipulate images "}]